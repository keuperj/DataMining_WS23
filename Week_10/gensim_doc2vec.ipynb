{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2ter7IP-XOg3"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "svWe0YMOXuNY",
    "outputId": "043ab25b-bf39-471a-ea30-fbfdd3ea1dd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (4.1.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "#update gensim\n",
    "!pip install --upgrade gensim\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-h1i3V5SXOg7"
   },
   "source": [
    "\n",
    "Doc2Vec Model\n",
    "=============\n",
    "\n",
    "Introduces Gensim's Doc2Vec model and demonstrates its use on the\n",
    "`Lee Corpus <https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`__.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zbQur-t8XOg_"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oho7olrSXOhA"
   },
   "source": [
    "Doc2Vec is a `core_concepts_model` that represents each\n",
    "`core_concepts_document` as a `core_concepts_vector`.  This\n",
    "tutorial introduces the model and demonstrates how to train and assess it.\n",
    "\n",
    "Here's a list of what we'll be doing:\n",
    "\n",
    "0. Review the relevant models: bag-of-words, Word2Vec, Doc2Vec\n",
    "1. Load and preprocess the training and test corpora (see `core_concepts_corpus`)\n",
    "2. Train a Doc2Vec `core_concepts_model` model using the training corpus\n",
    "3. Demonstrate how the trained model can be used to infer a `core_concepts_vector`\n",
    "4. Assess the model\n",
    "5. Test the model on the test corpus\n",
    "\n",
    "Review: Bag-of-words\n",
    "--------------------\n",
    "\n",
    ".. Note:: Feel free to skip these review sections if you're already familiar with the models.\n",
    "\n",
    "You may be familiar with the `bag-of-words model\n",
    "<https://en.wikipedia.org/wiki/Bag-of-words_model>`_ from the\n",
    "`core_concepts_vector` section.\n",
    "This model transforms each document to a fixed-length vector of integers.\n",
    "For example, given the sentences:\n",
    "\n",
    "- ``John likes to watch movies. Mary likes movies too.``\n",
    "- ``John also likes to watch football games. Mary hates football.``\n",
    "\n",
    "The model outputs the vectors:\n",
    "\n",
    "- ``[1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0]``\n",
    "- ``[1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1]``\n",
    "\n",
    "Each vector has 10 elements, where each element counts the number of times a\n",
    "particular word occurred in the document.\n",
    "The order of elements is arbitrary.\n",
    "In the example above, the order of the elements corresponds to the words:\n",
    "``[\"John\", \"likes\", \"to\", \"watch\", \"movies\", \"Mary\", \"too\", \"also\", \"football\", \"games\", \"hates\"]``.\n",
    "\n",
    "Bag-of-words models are surprisingly effective, but have several weaknesses.\n",
    "\n",
    "First, they lose all information about word order: \"John likes Mary\" and\n",
    "\"Mary likes John\" correspond to identical vectors. There is a solution: bag\n",
    "of `n-grams <https://en.wikipedia.org/wiki/N-gram>`__\n",
    "models consider word phrases of length n to represent documents as\n",
    "fixed-length vectors to capture local word order but suffer from data\n",
    "sparsity and high dimensionality.\n",
    "\n",
    "Second, the model does not attempt to learn the meaning of the underlying\n",
    "words, and as a consequence, the distance between vectors doesn't always\n",
    "reflect the difference in meaning.  The ``Word2Vec`` model addresses this\n",
    "second problem.\n",
    "\n",
    "Review: ``Word2Vec`` Model\n",
    "--------------------------\n",
    "\n",
    "``Word2Vec`` is a more recent model that embeds words in a lower-dimensional\n",
    "vector space using a shallow neural network. The result is a set of\n",
    "word-vectors where vectors close together in vector space have similar\n",
    "meanings based on context, and word-vectors distant to each other have\n",
    "differing meanings. For example, ``strong`` and ``powerful`` would be close\n",
    "together and ``strong`` and ``Paris`` would be relatively far.\n",
    "\n",
    "Gensim's :py:class:`~gensim.models.word2vec.Word2Vec` class implements this model.\n",
    "\n",
    "With the ``Word2Vec`` model, we can calculate the vectors for each **word** in a document.\n",
    "But what if we want to calculate a vector for the **entire document**\\ ?\n",
    "We could average the vectors for each word in the document - while this is quick and crude, it can often be useful.\n",
    "However, there is a better way...\n",
    "\n",
    "Introducing: Paragraph Vector\n",
    "-----------------------------\n",
    "\n",
    ".. Important:: In Gensim, we refer to the Paragraph Vector model as ``Doc2Vec``.\n",
    "\n",
    "Le and Mikolov in 2014 introduced the `Doc2Vec algorithm <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`__,\n",
    "which usually outperforms such simple-averaging of ``Word2Vec`` vectors.\n",
    "\n",
    "The basic idea is: act as if a document has another floating word-like\n",
    "vector, which contributes to all training predictions, and is updated like\n",
    "other word-vectors, but we will call it a doc-vector. Gensim's\n",
    ":py:class:`~gensim.models.doc2vec.Doc2Vec` class implements this algorithm.\n",
    "\n",
    "There are two implementations:\n",
    "\n",
    "1. Paragraph Vector - Distributed Memory (PV-DM)\n",
    "2. Paragraph Vector - Distributed Bag of Words (PV-DBOW)\n",
    "\n",
    ".. Important::\n",
    "  Don't let the implementation details below scare you.\n",
    "  They're advanced material: if it's too much, then move on to the next section.\n",
    "\n",
    "PV-DM is analogous to Word2Vec CBOW. The doc-vectors are obtained by training\n",
    "a neural network on the synthetic task of predicting a center word based an\n",
    "average of both context word-vectors and the full document's doc-vector.\n",
    "\n",
    "PV-DBOW is analogous to Word2Vec SG. The doc-vectors are obtained by training\n",
    "a neural network on the synthetic task of predicting a target word just from\n",
    "the full document's doc-vector. (It is also common to combine this with\n",
    "skip-gram testing, using both the doc-vector and nearby word-vectors to\n",
    "predict a single target word, but only one at a time.)\n",
    "\n",
    "Prepare the Training and Test Data\n",
    "----------------------------------\n",
    "\n",
    "For this tutorial, we'll be training our model using the `Lee Background\n",
    "Corpus\n",
    "<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_\n",
    "included in gensim. This corpus contains 314 documents selected from the\n",
    "Australian Broadcasting Corporationâ€™s news mail service, which provides text\n",
    "e-mails of headline stories and covers a number of broad topics.\n",
    "\n",
    "And we'll test our model by eye using the much shorter `Lee Corpus\n",
    "<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_\n",
    "which contains 50 documents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "a2HkUp9uXOhY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "# Set file names for train and test data\n",
    "test_data_dir = os.path.join(gensim.__path__[0], 'test', 'test_data')\n",
    "lee_train_file = os.path.join(test_data_dir, 'lee_background.cor')\n",
    "lee_test_file = os.path.join(test_data_dir, 'lee.cor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtcdVY_eXOhZ"
   },
   "source": [
    "Define a Function to Read and Preprocess Text\n",
    "---------------------------------------------\n",
    "\n",
    "Below, we define a function to:\n",
    "\n",
    "- open the train/test file (with latin encoding)\n",
    "- read the file line-by-line\n",
    "- pre-process each line (tokenize text into individual words, remove punctuation, set to lowercase, etc)\n",
    "\n",
    "The file we're reading is a **corpus**.\n",
    "Each line of the file is a **document**.\n",
    "\n",
    ".. Important::\n",
    "  To train the model, we'll need to associate a tag/number with each document\n",
    "  of the training corpus. In our case, the tag is simply the zero-based line\n",
    "  number.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WDbRi3AQXOha"
   },
   "outputs": [],
   "source": [
    "import smart_open\n",
    "\n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            tokens = gensim.utils.simple_preprocess(line)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "\n",
    "train_corpus = list(read_corpus(lee_train_file))\n",
    "test_corpus = list(read_corpus(lee_test_file, tokens_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQhuIzMsXOhb"
   },
   "source": [
    "Let's take a look at the training corpus\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Vrsq3F3XOhc",
    "outputId": "604d2ea3-ca4e-4628-fa8e-462399b95310"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['hundreds', 'of', 'people', 'have', 'been', 'forced', 'to', 'vacate', 'their', 'homes', 'in', 'the', 'southern', 'highlands', 'of', 'new', 'south', 'wales', 'as', 'strong', 'winds', 'today', 'pushed', 'huge', 'bushfire', 'towards', 'the', 'town', 'of', 'hill', 'top', 'new', 'blaze', 'near', 'goulburn', 'south', 'west', 'of', 'sydney', 'has', 'forced', 'the', 'closure', 'of', 'the', 'hume', 'highway', 'at', 'about', 'pm', 'aedt', 'marked', 'deterioration', 'in', 'the', 'weather', 'as', 'storm', 'cell', 'moved', 'east', 'across', 'the', 'blue', 'mountains', 'forced', 'authorities', 'to', 'make', 'decision', 'to', 'evacuate', 'people', 'from', 'homes', 'in', 'outlying', 'streets', 'at', 'hill', 'top', 'in', 'the', 'new', 'south', 'wales', 'southern', 'highlands', 'an', 'estimated', 'residents', 'have', 'left', 'their', 'homes', 'for', 'nearby', 'mittagong', 'the', 'new', 'south', 'wales', 'rural', 'fire', 'service', 'says', 'the', 'weather', 'conditions', 'which', 'caused', 'the', 'fire', 'to', 'burn', 'in', 'finger', 'formation', 'have', 'now', 'eased', 'and', 'about', 'fire', 'units', 'in', 'and', 'around', 'hill', 'top', 'are', 'optimistic', 'of', 'defending', 'all', 'properties', 'as', 'more', 'than', 'blazes', 'burn', 'on', 'new', 'year', 'eve', 'in', 'new', 'south', 'wales', 'fire', 'crews', 'have', 'been', 'called', 'to', 'new', 'fire', 'at', 'gunning', 'south', 'of', 'goulburn', 'while', 'few', 'details', 'are', 'available', 'at', 'this', 'stage', 'fire', 'authorities', 'says', 'it', 'has', 'closed', 'the', 'hume', 'highway', 'in', 'both', 'directions', 'meanwhile', 'new', 'fire', 'in', 'sydney', 'west', 'is', 'no', 'longer', 'threatening', 'properties', 'in', 'the', 'cranebrook', 'area', 'rain', 'has', 'fallen', 'in', 'some', 'parts', 'of', 'the', 'illawarra', 'sydney', 'the', 'hunter', 'valley', 'and', 'the', 'north', 'coast', 'but', 'the', 'bureau', 'of', 'meteorology', 'claire', 'richards', 'says', 'the', 'rain', 'has', 'done', 'little', 'to', 'ease', 'any', 'of', 'the', 'hundred', 'fires', 'still', 'burning', 'across', 'the', 'state', 'the', 'falls', 'have', 'been', 'quite', 'isolated', 'in', 'those', 'areas', 'and', 'generally', 'the', 'falls', 'have', 'been', 'less', 'than', 'about', 'five', 'millimetres', 'she', 'said', 'in', 'some', 'places', 'really', 'not', 'significant', 'at', 'all', 'less', 'than', 'millimetre', 'so', 'there', 'hasn', 'been', 'much', 'relief', 'as', 'far', 'as', 'rain', 'is', 'concerned', 'in', 'fact', 'they', 've', 'probably', 'hampered', 'the', 'efforts', 'of', 'the', 'firefighters', 'more', 'because', 'of', 'the', 'wind', 'gusts', 'that', 'are', 'associated', 'with', 'those', 'thunderstorms'], tags=[0]), TaggedDocument(words=['indian', 'security', 'forces', 'have', 'shot', 'dead', 'eight', 'suspected', 'militants', 'in', 'night', 'long', 'encounter', 'in', 'southern', 'kashmir', 'the', 'shootout', 'took', 'place', 'at', 'dora', 'village', 'some', 'kilometers', 'south', 'of', 'the', 'kashmiri', 'summer', 'capital', 'srinagar', 'the', 'deaths', 'came', 'as', 'pakistani', 'police', 'arrested', 'more', 'than', 'two', 'dozen', 'militants', 'from', 'extremist', 'groups', 'accused', 'of', 'staging', 'an', 'attack', 'on', 'india', 'parliament', 'india', 'has', 'accused', 'pakistan', 'based', 'lashkar', 'taiba', 'and', 'jaish', 'mohammad', 'of', 'carrying', 'out', 'the', 'attack', 'on', 'december', 'at', 'the', 'behest', 'of', 'pakistani', 'military', 'intelligence', 'military', 'tensions', 'have', 'soared', 'since', 'the', 'raid', 'with', 'both', 'sides', 'massing', 'troops', 'along', 'their', 'border', 'and', 'trading', 'tit', 'for', 'tat', 'diplomatic', 'sanctions', 'yesterday', 'pakistan', 'announced', 'it', 'had', 'arrested', 'lashkar', 'taiba', 'chief', 'hafiz', 'mohammed', 'saeed', 'police', 'in', 'karachi', 'say', 'it', 'is', 'likely', 'more', 'raids', 'will', 'be', 'launched', 'against', 'the', 'two', 'groups', 'as', 'well', 'as', 'other', 'militant', 'organisations', 'accused', 'of', 'targetting', 'india', 'military', 'tensions', 'between', 'india', 'and', 'pakistan', 'have', 'escalated', 'to', 'level', 'not', 'seen', 'since', 'their', 'war'], tags=[1])]\n"
     ]
    }
   ],
   "source": [
    "print(train_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YD24UUfXOhd"
   },
   "source": [
    "And the testing corpus looks like this:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "559GGBA-XOhe",
    "outputId": "0f61eb57-d6b6-4927-e457-8953951c4b7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'national', 'executive', 'of', 'the', 'strife', 'torn', 'democrats', 'last', 'night', 'appointed', 'little', 'known', 'west', 'australian', 'senator', 'brian', 'greig', 'as', 'interim', 'leader', 'shock', 'move', 'likely', 'to', 'provoke', 'further', 'conflict', 'between', 'the', 'party', 'senators', 'and', 'its', 'organisation', 'in', 'move', 'to', 'reassert', 'control', 'over', 'the', 'party', 'seven', 'senators', 'the', 'national', 'executive', 'last', 'night', 'rejected', 'aden', 'ridgeway', 'bid', 'to', 'become', 'interim', 'leader', 'in', 'favour', 'of', 'senator', 'greig', 'supporter', 'of', 'deposed', 'leader', 'natasha', 'stott', 'despoja', 'and', 'an', 'outspoken', 'gay', 'rights', 'activist'], ['cash', 'strapped', 'financial', 'services', 'group', 'amp', 'has', 'shelved', 'million', 'plan', 'to', 'buy', 'shares', 'back', 'from', 'investors', 'and', 'will', 'raise', 'million', 'in', 'fresh', 'capital', 'after', 'profits', 'crashed', 'in', 'the', 'six', 'months', 'to', 'june', 'chief', 'executive', 'paul', 'batchelor', 'said', 'the', 'result', 'was', 'solid', 'in', 'what', 'he', 'described', 'as', 'the', 'worst', 'conditions', 'for', 'stock', 'markets', 'in', 'years', 'amp', 'half', 'year', 'profit', 'sank', 'per', 'cent', 'to', 'million', 'or', 'share', 'as', 'australia', 'largest', 'investor', 'and', 'fund', 'manager', 'failed', 'to', 'hit', 'projected', 'per', 'cent', 'earnings', 'growth', 'targets', 'and', 'was', 'battered', 'by', 'falling', 'returns', 'on', 'share', 'markets']]\n"
     ]
    }
   ],
   "source": [
    "print(test_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92q6JFy7XOhe"
   },
   "source": [
    "Notice that the testing corpus is just a list of lists and does not contain\n",
    "any tags.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzVRia6-XOhf"
   },
   "source": [
    "Training the Model\n",
    "------------------\n",
    "\n",
    "Now, we'll instantiate a Doc2Vec model with a vector size with 50 dimensions and\n",
    "iterating over the training corpus 40 times. We set the minimum word count to\n",
    "2 in order to discard words with very few occurrences. (Without a variety of\n",
    "representative examples, retaining such infrequent words can often make a\n",
    "model worse!) Typical iteration counts in the published `Paragraph Vector paper <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`__\n",
    "results, using 10s-of-thousands to millions of docs, are 10-20. More\n",
    "iterations take more time and eventually reach a point of diminishing\n",
    "returns.\n",
    "\n",
    "However, this is a very very small dataset (300 documents) with shortish\n",
    "documents (a few hundred words). Adding training passes can sometimes help\n",
    "with such small datasets.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TCM9vSAKXOhg",
    "outputId": "02c6d5b4-d644-4877-886d-c89544aa1a0e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 08:04:41,633 : INFO : Doc2Vec lifecycle event {'params': 'Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3)', 'datetime': '2021-12-10T08:04:41.633719', 'gensim': '4.1.2', 'python': '3.7.12 (default, Sep 10 2021, 00:21:48) \\n[GCC 7.5.0]', 'platform': 'Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrwSyuxoXOhg"
   },
   "source": [
    "Build a vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JpJiP2B7XOhh",
    "outputId": "3bb3d3ba-e344-48c2-dfce-22380a239a1f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 08:04:41,645 : INFO : collecting all words and their counts\n",
      "2021-12-10 08:04:41,647 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2021-12-10 08:04:41,672 : INFO : collected 6981 word types and 300 unique tags from a corpus of 300 examples and 58152 words\n",
      "2021-12-10 08:04:41,674 : INFO : Creating a fresh vocabulary\n",
      "2021-12-10 08:04:41,712 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 3955 unique words (56.653774530869505%% of original 6981, drops 3026)', 'datetime': '2021-12-10T08:04:41.712602', 'gensim': '4.1.2', 'python': '3.7.12 (default, Sep 10 2021, 00:21:48) \\n[GCC 7.5.0]', 'platform': 'Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic', 'event': 'prepare_vocab'}\n",
      "2021-12-10 08:04:41,715 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 55126 word corpus (94.79639565277205%% of original 58152, drops 3026)', 'datetime': '2021-12-10T08:04:41.715152', 'gensim': '4.1.2', 'python': '3.7.12 (default, Sep 10 2021, 00:21:48) \\n[GCC 7.5.0]', 'platform': 'Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic', 'event': 'prepare_vocab'}\n",
      "2021-12-10 08:04:41,756 : INFO : deleting the raw counts dictionary of 6981 items\n",
      "2021-12-10 08:04:41,758 : INFO : sample=0.001 downsamples 46 most-common words\n",
      "2021-12-10 08:04:41,759 : INFO : Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 42390.98914085061 word corpus (76.9%% of prior 55126)', 'datetime': '2021-12-10T08:04:41.759943', 'gensim': '4.1.2', 'python': '3.7.12 (default, Sep 10 2021, 00:21:48) \\n[GCC 7.5.0]', 'platform': 'Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic', 'event': 'prepare_vocab'}\n",
      "2021-12-10 08:04:41,815 : INFO : estimated required memory for 3955 words and 50 dimensions: 3679500 bytes\n",
      "2021-12-10 08:04:41,817 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdXp33oVXOhh"
   },
   "source": [
    "Essentially, the vocabulary is a list (accessible via\n",
    "``model.wv.index_to_key``) of all of the unique words extracted from the training corpus.\n",
    "Additional attributes for each word are available using the ``model.wv.get_vecattr()`` method,\n",
    "For example, to see how many times ``penalty`` appeared in the training corpus:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P2ZqUxQlXOhi",
    "outputId": "9dd0e957-5078-4f85-a7dc-cf5ab38e6f8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'penalty' appeared 4 times in the training corpus.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Word 'penalty' appeared {model.wv.get_vecattr('penalty', 'count')} times in the training corpus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEx_s0B6XOhi"
   },
   "source": [
    "Next, train the model on the corpus.\n",
    "If optimized Gensim (with BLAS library) is being used, this should take no more than 3 seconds.\n",
    "If the BLAS library is not being used, this should take no more than 2\n",
    "minutes, so use optimized Gensim with BLAS if you value your time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SKYhNiTBXOhj",
    "outputId": "028474bc-3b49-4a83-ee91-b7b0d6d62d25"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 08:04:41,846 : INFO : Doc2Vec lifecycle event {'msg': 'training model with 3 workers on 3955 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2021-12-10T08:04:41.846219', 'gensim': '4.1.2', 'python': '3.7.12 (default, Sep 10 2021, 00:21:48) \\n[GCC 7.5.0]', 'platform': 'Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic', 'event': 'train'}\n",
      "2021-12-10 08:04:41,959 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:41,966 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:41,978 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:41,979 : INFO : EPOCH - 1 : training on 58152 raw words (42651 effective words) took 0.1s, 333151 effective words/s\n",
      "2021-12-10 08:04:42,088 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:42,109 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:42,116 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:42,117 : INFO : EPOCH - 2 : training on 58152 raw words (42698 effective words) took 0.1s, 327151 effective words/s\n",
      "2021-12-10 08:04:42,230 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:42,238 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:42,250 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:42,251 : INFO : EPOCH - 3 : training on 58152 raw words (42746 effective words) took 0.1s, 342713 effective words/s\n",
      "2021-12-10 08:04:42,367 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:42,379 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:42,397 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:42,399 : INFO : EPOCH - 4 : training on 58152 raw words (42720 effective words) took 0.1s, 300100 effective words/s\n",
      "2021-12-10 08:04:42,524 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:42,541 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:42,543 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:42,544 : INFO : EPOCH - 5 : training on 58152 raw words (42686 effective words) took 0.1s, 307332 effective words/s\n",
      "2021-12-10 08:04:42,647 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:42,669 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:42,671 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:42,673 : INFO : EPOCH - 6 : training on 58152 raw words (42664 effective words) took 0.1s, 353436 effective words/s\n",
      "2021-12-10 08:04:42,789 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:42,808 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:42,819 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:42,822 : INFO : EPOCH - 7 : training on 58152 raw words (42708 effective words) took 0.1s, 309344 effective words/s\n",
      "2021-12-10 08:04:42,930 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:42,938 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:42,946 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:42,947 : INFO : EPOCH - 8 : training on 58152 raw words (42746 effective words) took 0.1s, 358831 effective words/s\n",
      "2021-12-10 08:04:43,059 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:43,081 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:43,088 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:43,089 : INFO : EPOCH - 9 : training on 58152 raw words (42707 effective words) took 0.1s, 311083 effective words/s\n",
      "2021-12-10 08:04:43,200 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:43,211 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:43,218 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:43,219 : INFO : EPOCH - 10 : training on 58152 raw words (42677 effective words) took 0.1s, 346951 effective words/s\n",
      "2021-12-10 08:04:43,333 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:43,343 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:43,350 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:43,352 : INFO : EPOCH - 11 : training on 58152 raw words (42741 effective words) took 0.1s, 338575 effective words/s\n",
      "2021-12-10 08:04:43,446 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:43,464 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:43,484 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:43,485 : INFO : EPOCH - 12 : training on 58152 raw words (42778 effective words) took 0.1s, 337447 effective words/s\n",
      "2021-12-10 08:04:43,597 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:43,607 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:43,614 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:43,616 : INFO : EPOCH - 13 : training on 58152 raw words (42810 effective words) took 0.1s, 348806 effective words/s\n",
      "2021-12-10 08:04:43,707 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:43,736 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:43,742 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:43,744 : INFO : EPOCH - 14 : training on 58152 raw words (42644 effective words) took 0.1s, 348764 effective words/s\n",
      "2021-12-10 08:04:43,846 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:43,861 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:43,869 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:43,870 : INFO : EPOCH - 15 : training on 58152 raw words (42641 effective words) took 0.1s, 350758 effective words/s\n",
      "2021-12-10 08:04:43,981 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:43,987 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:43,995 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:43,998 : INFO : EPOCH - 16 : training on 58152 raw words (42798 effective words) took 0.1s, 350632 effective words/s\n",
      "2021-12-10 08:04:44,110 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:44,112 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:44,125 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:44,126 : INFO : EPOCH - 17 : training on 58152 raw words (42669 effective words) took 0.1s, 347002 effective words/s\n",
      "2021-12-10 08:04:44,234 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:44,259 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:44,269 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:44,271 : INFO : EPOCH - 18 : training on 58152 raw words (42686 effective words) took 0.1s, 310613 effective words/s\n",
      "2021-12-10 08:04:44,375 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:44,386 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:44,396 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:44,398 : INFO : EPOCH - 19 : training on 58152 raw words (42721 effective words) took 0.1s, 348216 effective words/s\n",
      "2021-12-10 08:04:44,514 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:44,521 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:44,526 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:44,527 : INFO : EPOCH - 20 : training on 58152 raw words (42646 effective words) took 0.1s, 344535 effective words/s\n",
      "2021-12-10 08:04:44,641 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:44,649 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:44,652 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:44,655 : INFO : EPOCH - 21 : training on 58152 raw words (42768 effective words) took 0.1s, 350765 effective words/s\n",
      "2021-12-10 08:04:44,755 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:44,776 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:44,784 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:44,785 : INFO : EPOCH - 22 : training on 58152 raw words (42695 effective words) took 0.1s, 345099 effective words/s\n",
      "2021-12-10 08:04:44,890 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:44,913 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:44,915 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:44,920 : INFO : EPOCH - 23 : training on 58152 raw words (42756 effective words) took 0.1s, 357957 effective words/s\n",
      "2021-12-10 08:04:45,016 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:45,040 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:45,047 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:45,048 : INFO : EPOCH - 24 : training on 58152 raw words (42714 effective words) took 0.1s, 345023 effective words/s\n",
      "2021-12-10 08:04:45,159 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:45,173 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:45,178 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:45,180 : INFO : EPOCH - 25 : training on 58152 raw words (42645 effective words) took 0.1s, 337316 effective words/s\n",
      "2021-12-10 08:04:45,290 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:45,296 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:45,304 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:45,306 : INFO : EPOCH - 26 : training on 58152 raw words (42834 effective words) took 0.1s, 357245 effective words/s\n",
      "2021-12-10 08:04:45,418 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:45,429 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:45,437 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:45,439 : INFO : EPOCH - 27 : training on 58152 raw words (42753 effective words) took 0.1s, 335456 effective words/s\n",
      "2021-12-10 08:04:45,538 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:45,557 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:45,559 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:45,560 : INFO : EPOCH - 28 : training on 58152 raw words (42698 effective words) took 0.1s, 369538 effective words/s\n",
      "2021-12-10 08:04:45,666 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:45,676 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:45,687 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:45,689 : INFO : EPOCH - 29 : training on 58152 raw words (42819 effective words) took 0.1s, 354087 effective words/s\n",
      "2021-12-10 08:04:45,808 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:45,816 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:45,824 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:45,825 : INFO : EPOCH - 30 : training on 58152 raw words (42706 effective words) took 0.1s, 330831 effective words/s\n",
      "2021-12-10 08:04:45,921 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:45,936 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:45,953 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:45,955 : INFO : EPOCH - 31 : training on 58152 raw words (42613 effective words) took 0.1s, 349070 effective words/s\n",
      "2021-12-10 08:04:46,073 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:46,076 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:46,088 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:46,089 : INFO : EPOCH - 32 : training on 58152 raw words (42668 effective words) took 0.1s, 330137 effective words/s\n",
      "2021-12-10 08:04:46,197 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:46,203 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:46,211 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:46,212 : INFO : EPOCH - 33 : training on 58152 raw words (42657 effective words) took 0.1s, 362408 effective words/s\n",
      "2021-12-10 08:04:46,331 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:46,336 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:46,338 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:46,342 : INFO : EPOCH - 34 : training on 58152 raw words (42597 effective words) took 0.1s, 343838 effective words/s\n",
      "2021-12-10 08:04:46,454 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:46,462 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:46,472 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:46,474 : INFO : EPOCH - 35 : training on 58152 raw words (42660 effective words) took 0.1s, 335696 effective words/s\n",
      "2021-12-10 08:04:46,575 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:46,585 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:46,594 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:46,596 : INFO : EPOCH - 36 : training on 58152 raw words (42651 effective words) took 0.1s, 365413 effective words/s\n",
      "2021-12-10 08:04:46,717 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:46,728 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:46,733 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:46,734 : INFO : EPOCH - 37 : training on 58152 raw words (42728 effective words) took 0.1s, 345694 effective words/s\n",
      "2021-12-10 08:04:46,843 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:46,860 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:46,868 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:46,870 : INFO : EPOCH - 38 : training on 58152 raw words (42649 effective words) took 0.1s, 329486 effective words/s\n",
      "2021-12-10 08:04:46,981 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:46,987 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:46,994 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:46,995 : INFO : EPOCH - 39 : training on 58152 raw words (42639 effective words) took 0.1s, 354827 effective words/s\n",
      "2021-12-10 08:04:47,094 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-10 08:04:47,110 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-10 08:04:47,126 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-10 08:04:47,128 : INFO : EPOCH - 40 : training on 58152 raw words (42631 effective words) took 0.1s, 334820 effective words/s\n",
      "2021-12-10 08:04:47,130 : INFO : Doc2Vec lifecycle event {'msg': 'training on 2326080 raw words (1708018 effective words) took 5.3s, 323378 effective words/s', 'datetime': '2021-12-10T08:04:47.130108', 'gensim': '4.1.2', 'python': '3.7.12 (default, Sep 10 2021, 00:21:48) \\n[GCC 7.5.0]', 'platform': 'Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic', 'event': 'train'}\n"
     ]
    }
   ],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJjMlORDXOhj"
   },
   "source": [
    "Now, we can use the trained model to infer a vector for any piece of text\n",
    "by passing a list of words to the ``model.infer_vector`` function. This\n",
    "vector can then be compared with other vectors via cosine similarity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SphLN1maXOhk",
    "outputId": "573af745-944c-4c0b-c086-e3795b73fb91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.30596519e-01 -2.60147840e-01 -8.15001205e-02  2.44218886e-01\n",
      " -6.01974316e-02  3.14293690e-02  1.18657477e-01  7.91448206e-02\n",
      " -4.14002538e-01 -1.91580966e-01  1.12926118e-01  1.35999396e-01\n",
      " -2.85161827e-02 -5.19467443e-02 -1.08953364e-01  1.93100162e-02\n",
      "  1.91469312e-01  2.93243229e-01  1.10384636e-01 -1.90839246e-01\n",
      " -4.63094935e-03 -3.44104730e-02  1.96451545e-01  2.44545052e-03\n",
      "  2.01078355e-02 -1.08346958e-02 -3.60417634e-01 -5.43140955e-02\n",
      " -2.22886369e-01 -1.61440700e-01  4.45364952e-01  1.26644745e-01\n",
      "  1.47747964e-01  1.82247669e-01  1.55548289e-01  1.10801451e-01\n",
      " -4.09679003e-02 -4.03157741e-01  8.30266144e-05 -8.05033818e-02\n",
      "  1.54651895e-01  1.31375995e-02 -2.59234328e-02 -8.81515741e-02\n",
      "  1.70529574e-01  7.62206391e-02 -6.77439645e-02 -2.10539013e-01\n",
      "  7.11581856e-02  4.77716234e-03]\n"
     ]
    }
   ],
   "source": [
    "vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHV0_l9rXOhk"
   },
   "source": [
    "Note that ``infer_vector()`` does *not* take a string, but rather a list of\n",
    "string tokens, which should have already been tokenized the same way as the\n",
    "``words`` property of original training document objects.\n",
    "\n",
    "Also note that because the underlying training/inference algorithms are an\n",
    "iterative approximation problem that makes use of internal randomization,\n",
    "repeated inferences of the same text will return slightly different vectors.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_asYQdVeXOhl"
   },
   "source": [
    "Assessing the Model\n",
    "-------------------\n",
    "\n",
    "To assess our new model, we'll first infer new vectors for each document of\n",
    "the training corpus, compare the inferred vectors with the training corpus,\n",
    "and then returning the rank of the document based on self-similarity.\n",
    "Basically, we're pretending as if the training corpus is some new unseen data\n",
    "and then seeing how they compare with the trained model. The expectation is\n",
    "that we've likely overfit our model (i.e., all of the ranks will be less than\n",
    "2) and so we should be able to find similar documents very easily.\n",
    "Additionally, we'll keep track of the second ranks for a comparison of less\n",
    "similar documents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "OyFd4hAsXOhl"
   },
   "outputs": [],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zg-i_F-fXOhm"
   },
   "source": [
    "Let's count how each document ranks with respect to the training corpus\n",
    "\n",
    "NB. Results vary between runs due to random seeding and very small corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qo1l9VpbXOhm",
    "outputId": "c406de63-77d4-4852-b7b2-b37187b2c451"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 293, 1: 7})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqByTiT-XOhm"
   },
   "source": [
    "Basically, greater than 95% of the inferred documents are found to be most\n",
    "similar to itself and about 5% of the time it is mistakenly most similar to\n",
    "another document. Checking the inferred-vector against a\n",
    "training-vector is a sort of 'sanity check' as to whether the model is\n",
    "behaving in a usefully consistent manner, though not a real 'accuracy' value.\n",
    "\n",
    "This is great and not entirely surprising. We can take a look at an example:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tOPmnAHkXOhn",
    "outputId": "b047e154-b0e6-4e8f-f07a-8a2c2f3a0613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document (299): Â«australia will take on france in the doubles rubber of the davis cup tennis final today with the tie levelled at wayne arthurs and todd woodbridge are scheduled to lead australia in the doubles against cedric pioline and fabrice santoro however changes can be made to the line up up to an hour before the match and australian team captain john fitzgerald suggested he might do just that we ll make team appraisal of the whole situation go over the pros and cons and make decision french team captain guy forget says he will not make changes but does not know what to expect from australia todd is the best doubles player in the world right now so expect him to play he said would probably use wayne arthurs but don know what to expect really pat rafter salvaged australia davis cup campaign yesterday with win in the second singles match rafter overcame an arm injury to defeat french number one sebastien grosjean in three sets the australian says he is happy with his form it not very pretty tennis there isn too many consistent bounces you are playing like said bit of classic old grass court rafter said rafter levelled the score after lleyton hewitt shock five set loss to nicholas escude in the first singles rubber but rafter says he felt no added pressure after hewitt defeat knew had good team to back me up even if we were down he said knew could win on the last day know the boys can win doubles so even if we were down still feel we are good enough team to win and vice versa they are good enough team to beat us as wellÂ»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (299, 0.9484854340553284): Â«australia will take on france in the doubles rubber of the davis cup tennis final today with the tie levelled at wayne arthurs and todd woodbridge are scheduled to lead australia in the doubles against cedric pioline and fabrice santoro however changes can be made to the line up up to an hour before the match and australian team captain john fitzgerald suggested he might do just that we ll make team appraisal of the whole situation go over the pros and cons and make decision french team captain guy forget says he will not make changes but does not know what to expect from australia todd is the best doubles player in the world right now so expect him to play he said would probably use wayne arthurs but don know what to expect really pat rafter salvaged australia davis cup campaign yesterday with win in the second singles match rafter overcame an arm injury to defeat french number one sebastien grosjean in three sets the australian says he is happy with his form it not very pretty tennis there isn too many consistent bounces you are playing like said bit of classic old grass court rafter said rafter levelled the score after lleyton hewitt shock five set loss to nicholas escude in the first singles rubber but rafter says he felt no added pressure after hewitt defeat knew had good team to back me up even if we were down he said knew could win on the last day know the boys can win doubles so even if we were down still feel we are good enough team to win and vice versa they are good enough team to beat us as wellÂ»\n",
      "\n",
      "SECOND-MOST (112, 0.7849922180175781): Â«australian cricket captain steve waugh has supported fast bowler brett lee after criticism of his intimidatory bowling to the south african tailenders in the first test in adelaide earlier this month lee was fined for giving new zealand tailender shane bond an unsportsmanlike send off during the third test in perth waugh says tailenders should not be protected from short pitched bowling these days you re earning big money you ve got responsibility to learn how to bat he said mean there no times like years ago when it was not professional and sort of bowlers code these days you re professional our batsmen work very hard at their batting and expect other tailenders to do likewise meanwhile waugh says his side will need to guard against complacency after convincingly winning the first test by runs waugh says despite the dominance of his side in the first test south africa can never be taken lightly it only one test match out of three or six whichever way you want to look at it so there lot of work to go he said but it nice to win the first battle definitely it gives us lot of confidence going into melbourne you know the big crowd there we love playing in front of the boxing day crowd so that will be to our advantage as well south africa begins four day match against new south wales in sydney on thursday in the lead up to the boxing day test veteran fast bowler allan donald will play in the warm up match and is likely to take his place in the team for the second test south african captain shaun pollock expects much better performance from his side in the melbourne test we still believe that we didn play to our full potential so if we can improve on our aspects the output we put out on the field will be lot better and we still believe we have side that is good enough to beat australia on our day he saidÂ»\n",
      "\n",
      "MEDIAN (163, 0.24601487815380096): Â«the secret australian budget for the boat people pacific solution is set at to million the estimate approved by cabinet in september when nauru first agreed to house and process asylum seekers was based on joint submission from immigration minister philip ruddock the foreign minister alexander downer and the then defence minister peter reith it covered everything from the defence costs in transporting boat people to health care camp construction and guarding asylum seekers the participation of at least three government departments means the costs of setting up the boat people camp in nauru can be allocated across different portfolios some of the first distribution of cash was used to meet nauru unpaid bills australia is pushing on with the pacific solution with agreement yesterday to boost the number of boat people places in nauru from people toÂ»\n",
      "\n",
      "LEAST (216, -0.1343918740749359): Â«senior taliban official confirmed the islamic militia would begin handing over its last bastion of kandahar to pashtun tribal leaders on friday this agreement was that taliban should surrender kandahar peacefully to the elders of these areas and we should guarantee the lives and the safety of taliban authorities and all the taliban from tomorrow should start this program former taliban ambassador to pakistan abdul salam zaeef told cnn in telephone interview he insisted that the taliban would not surrender to hamid karzai the new afghan interim leader and pashtun elder who has been cooperating with the united states to calm unrest among the southern tribes the taliban will surrender to elders not to karzai karzai and other persons which they want to enter kandahar by the support of america they don allow to enter kandahar city he said the taliban will surrender the weapons the ammunition to eldersÂ»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Document ({}): Â«{}Â»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: Â«%sÂ»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNmiue0WXOhn"
   },
   "source": [
    "Notice above that the most similar document (usually the same text) is has a\n",
    "similarity score approaching 1.0. However, the similarity score for the\n",
    "second-ranked documents should be significantly lower (assuming the documents\n",
    "are in fact different) and the reasoning becomes obvious when we examine the\n",
    "text itself.\n",
    "\n",
    "We can run the next cell repeatedly to see a sampling other target-document\n",
    "comparisons.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJYd5s2OXOhn",
    "outputId": "c1719000-7eb0-443f-cdce-c3948d195c95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Document (4): Â«six midwives have been suspended at wollongong hospital south of sydney for inappropriate use of nitrous oxide during work hours on some occasions while women were in labour the illawarra area health service says that following an investigation of unprofessional conduct further four midwives have been relocated to other areas within the hospital the service chief executive officer tony sherbon says no one was put at risk because other staff not involved in the use of nitrous oxide were able to take over caring for women in labour well we re very concerned and the body of midwives to the hospital there are over midwives that work in our service are very annoyed and angry at the inappropriate behaviour of these very senior people who should know better he said and that why we ve take the action of suspending them and we ll consider further action next weekÂ»\n",
      "\n",
      "Similar Document (33, 0.7164185643196106): Â«new south wales firefighters are hoping lighter winds will help ease their workload today but are predicting nasty conditions over the weekend while the winds are expected to ease somewhat today the weather bureau says temperatures will be higher more than fires are still burning across new south wales the rural fire service says the change may allow it to concentrate more on preventative action but there is no room for complacency mark sullivan from the rural fire service says while conditions may be little kinder to them today the outlook for the weekend has them worried it certainly appears from the weather forecast with very high temperatures and high winds that it certainly could be nasty couple of days ahead mr sullivan said one of the areas causing greatest concern today is the kilometre long blaze in the lower blue mountains firefighters are also keeping close eye on blaze at spencer north of sydney which yesterday broke through containment lines there are concerns that fire may jump the hawkesbury river backburning continues in the state central west and south of sydney in the shoalhaven in the illawarra firefighters have been able to carry out back burning operations in three areas operations were carried out in parts of mt kembla as well as an area bounded by appin road and the old princes highway at helensburgh an area west of windy gully near cataract dam was also targeted meanwhile illawarra police have arrested three teenagers in relation to bushfires at shellharbour on the south coast of new south wales spokesman says three small fires were extinguished around pm aedt yesterday short time later police arrested three year old boys from shellharbour barrack heights and shell cove all three have been interviewed but no charges have been laidÂ»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the corpus and infer a vector from the model\n",
    "import random\n",
    "doc_id = random.randint(0, len(train_corpus) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): Â«{}Â»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: Â«{}Â»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvHT5WgsXOho"
   },
   "source": [
    "Testing the Model\n",
    "-----------------\n",
    "\n",
    "Using the same approach above, we'll infer the vector for a randomly chosen\n",
    "test document, and compare the document to our model by eye.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u0HppMluXOho",
    "outputId": "f87929e0-d146-4d43-91bb-1b92bc3a76c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (23): Â«china said sunday it issued new regulations controlling the export of missile technology taking steps to ease concerns about transferring sensitive equipment to middle east countries particularly iran however the new rules apparently do not ban outright the transfer of specific items something washington long has urged beijing to doÂ»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (186, 0.6075350046157837): Â«united nationals secretary general kofi annan has accepted the nobel peace prize in the norwegian capital oslo declaring that to save one life is to save humanity itself mr annan told gala audience the world must respect the individual whose fundamental rights he says have been sacrificed too often for the good of the state the year old un chief native of ghana shares this year th nobel peace prize with the united nations as whole his award was for bringing new life to the world body in his fight for human rights and against aids and terrorismÂ»\n",
      "\n",
      "MEDIAN (125, 0.31619271636009216): Â«the united states space shuttle endeavour has touched down at florida kennedy space centre after day mission bringing home crew that had been on the international space station since august the shuttle carrying outgoing space station commander frank culbertson and russian cosmonauts vladimir dezhurov and mikhail tyurin along with four other astronauts landed at pm local time taking over from the trio are russian commander yuri onufrienko and us astronauts carl walz and dan bursch who travelled to the station aboard endeavour on december earlier monday the seven us and russian astronauts on board endeavour woke up to the tune please come home for christmas by the rock group bon jovi on sunday the endeavour crew deployed small satellite called starshine from canister located in the shuttle payload bay more than students from countries will track the satellite as it orbits earth for the next eight months the students will collect information in order to calculate the density of the upper atmosphere nasa said on saturday endeavour undocked from the space station after making last minute maneuver to dodge piece of soviet era space refuse the endeavour mission the th shuttle trip to the international space station brought some three tonnes of equipment and materials for scientific experiments to the station the trip carried out under extremely tight security was the first since the september attacks on the united statesÂ»\n",
      "\n",
      "LEAST (157, -0.01564890146255493): Â«british man has been found guilty by unanimous verdict of the kidnap and murder of an eight year old schoolgirl whose death in july shocked britain and set off rampage of anti paedophile vigilantes roy whiting was sentenced to life imprisonment for the abduction and murder of eight year old sarah payne with recommendation by trial judge justice richard curtis that he never be released you are indeed an evil man you are in no way mentally unwell have seen you for month and in my view you are glib and cunning liar justice curtis said there were cheers of delight as the verdicts were read out by the foreman at lewes crown court the jury of nine men and three women had been deliberating for nine hours as soon as the verdicts were declared the court heard details of whiting previous conviction for the kidnap and indecent assault of nine year old girl in prosecutor timothy langdale told the jury how the defendant threw the child into the back of his dirty red ford sierra and locked the doors he had driven her somewhere she didn know where when she asked where they were going he said shut up because he had knife mr langdale said the defendant told the girl to take off her clothes when she refused he produced rope from his pocket and threatened to tie her up what he actually threatened was that he would tie her mouth up she took her clothes off as he had ordered her to do mr langdale then gave graphic details of the abuse to which whiting subjected the terrified child whiting was given four year jail sentence in june after admitting carrying out the attack in march that year but he was released in november despite warnings from probation officers who were convinced there was danger he would attack another child they set out their warnings in pre sentence report prepared after the first assault and in the parole report before he was released from prison he was kept under supervision for four months after his release but was not being monitored by july last year when eight year old sarah was abducted and killed whiting has been arrested three times in connection with the case but the first and second times was released without being charged sarah disappeared on july last year prompting massive police search her partially buried naked body was found days later in field and police believe she was strangled or suffocatedÂ»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): Â«{}Â»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: Â«%sÂ»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOjrzLPlXOho"
   },
   "source": [
    "Conclusion\n",
    "----------\n",
    "\n",
    "Let's review what we've seen in this tutorial:\n",
    "\n",
    "0. Review the relevant models: bag-of-words, Word2Vec, Doc2Vec\n",
    "1. Load and preprocess the training and test corpora (see `core_concepts_corpus`)\n",
    "2. Train a Doc2Vec `core_concepts_model` model using the training corpus\n",
    "3. Demonstrate how the trained model can be used to infer a `core_concepts_vector`\n",
    "4. Assess the model\n",
    "5. Test the model on the test corpus\n",
    "\n",
    "That's it! Doc2Vec is a great way to explore relationships between documents.\n",
    "\n",
    "Additional Resources\n",
    "--------------------\n",
    "\n",
    "If you'd like to know more about the subject matter of this tutorial, check out the links below.\n",
    "\n",
    "* `Word2Vec Paper <https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf>`_\n",
    "* `Doc2Vec Paper <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`_\n",
    "* `Dr. Michael D. Lee's Website <http://faculty.sites.uci.edu/mdlee>`_\n",
    "* `Lee Corpus <http://faculty.sites.uci.edu/mdlee/similarity-data/>`__\n",
    "* `IMDB Doc2Vec Tutorial <doc2vec-IMDB.ipynb>`_\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "gensim_doc2vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
